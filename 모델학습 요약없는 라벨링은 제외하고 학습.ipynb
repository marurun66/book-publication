{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "# ✅ 1️⃣ 학습 데이터 로드\n",
    "with open(\"labeling_novels_filtered.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    labeled_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 2️⃣ 데이터셋 변환\n",
    "train_data = [{\"input\": item[\"description\"], \"target\": item[\"summary\"]} for item in labeled_data]\n",
    "dataset = Dataset.from_dict({\n",
    "    \"input\": [item[\"input\"] for item in train_data],\n",
    "    \"target\": [item[\"target\"] for item in train_data]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "# ✅ 3️⃣ KoBART 모델 & 토크나이저 로드\n",
    "model_name = \"gogamza/kobart-summarization\"\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 4️⃣ 데이터 토크나이징 (Trainer가 이해할 수 있는 형태로 변환)\n",
    "def preprocess_data(example):\n",
    "    model_inputs = tokenizer(\n",
    "        example[\"input\"], \n",
    "        max_length=1024, \n",
    "        truncation=True, \n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    # 🔹 \"target\"을 \"labels\"로 변환 후 토크나이징\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            example[\"target\"], \n",
    "            max_length=250, \n",
    "            truncation=True, \n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "282ee381c1ca45ccb7711249e4911506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/217 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/hi/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3970: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ✅ 5️⃣ 데이터셋 변환 (batched=True로 속도 최적화)\n",
    "tokenized_dataset = dataset.map(preprocess_data, batched=True, remove_columns=[\"input\", \"target\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/hi/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ✅ 6️⃣ 학습 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./kobart_summary_finetuned\",\n",
    "    evaluation_strategy=\"epoch\",  # 🔹 매 epoch마다 평가\n",
    "    save_strategy=\"epoch\",        # 🔹 매 epoch마다 저장\n",
    "    per_device_train_batch_size=2,  # 🔹 배치 크기 (GPU 메모리에 따라 조정 가능)\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=5,  # 🔹 학습 횟수 (필요에 따라 조정 가능)\n",
    "    weight_decay=0.01,  # 🔹 가중치 감쇠 (과적합 방지)\n",
    "    save_total_limit=5,  # 🔹 체크포인트 최대 개수\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wz/cf10k4857715p70flsxw2g9m0000gn/T/ipykernel_2970/360371609.py:9: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# ✅ 1️⃣ 🤗 Datasets의 train_test_split() 사용\n",
    "dataset_split = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# ✅ 2️⃣ train/eval 데이터셋 설정\n",
    "train_data = dataset_split[\"train\"]\n",
    "eval_data = dataset_split[\"test\"]\n",
    "\n",
    "# ✅ 3️⃣ Trainer 객체 생성 (eval_dataset 추가)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,  # 🔹 평가 데이터셋 추가!\n",
    "    tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wz/cf10k4857715p70flsxw2g9m0000gn/T/ipykernel_2970/2867482875.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# ✅ 7️⃣ Trainer 객체 생성\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,   # ✅ 훈련 데이터\n",
    "    eval_dataset=eval_data,     # ✅ 평가 데이터 추가!\n",
    "    tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='435' max='435' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [435/435 03:12, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.817668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.033400</td>\n",
       "      <td>0.783941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.512600</td>\n",
       "      <td>0.816674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.289700</td>\n",
       "      <td>0.840684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.209200</td>\n",
       "      <td>0.862921</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=435, training_loss=0.7102061107240875, metrics={'train_runtime': 192.7244, 'train_samples_per_second': 4.488, 'train_steps_per_second': 2.257, 'total_flos': 527422036377600.0, 'train_loss': 0.7102061107240875, 'epoch': 5.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ✅ 8️⃣ 학습 시작 🚀\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ KoBART 파인튜닝 완료! 모델이 './kobart_summary_finetuned'에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# ✅ 9️⃣ 모델 저장\n",
    "model.save_pretrained(\"./kobart_summary_finetuned\")\n",
    "tokenizer.save_pretrained(\"./kobart_summary_finetuned\")\n",
    "\n",
    "print(\"✅ KoBART 파인튜닝 완료! 모델이 './kobart_summary_finetuned'에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 가장 좋은 모델이 './kobart_best_model'에 저장되었습니다!\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast\n",
    "\n",
    "best_checkpoint = \"./kobart_summary_finetuned/checkpoint-174\"  # epoch 2 모델 체크포인트\n",
    "save_path = \"./kobart_best_model\"\n",
    "\n",
    "# 모델과 토크나이저 불러오기\n",
    "model = BartForConditionalGeneration.from_pretrained(best_checkpoint)\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(best_checkpoint)\n",
    "\n",
    "# 저장\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(f\"✅ 가장 좋은 모델이 '{save_path}'에 저장되었습니다!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "/opt/anaconda3/envs/hi/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 원문:  이상주의자인 스노우볼과 음흉한 현실주의자인 나폴레온의 싸움은 당연히 나폴레온의 승리로 끝나고 만다.\n",
      " 나폴레온의 독재가 시작되었고, 이제는 철통 같은 규율만을 강조하는 것만으로는 부족했다.\n",
      " 무엇인가 새로운 자극을 주는 대상이 필요한 것이다.\n",
      " 그래서 나폴레온은 추출된 스노우볼을 이용한다.\n",
      " 이제 스노우볼은 모든 동물들로부터 증오의 표적이 된다.\n",
      " 농장에서의 실패는 모두 그의 탓으로 돌려진다.\n",
      " 심지어는 충성도가 떨어지는 동물들은 모두 스노우볼의 앞잡이로 처단된다.\n",
      "🔹 요약 결과:  이상주의자인 스노우볼과 음흉한 현실주의자인 나폴레온의 싸움은 결국 나폴레온의 승리로 끝이 나지만, 철통 같은 규율만을 강조하는 것만으로는 부족하고, 새로운 자극을 주는 대상이 필요한 상황이다.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast\n",
    "\n",
    "# ✅ 저장된 모델 불러오기\n",
    "best_checkpoint = \"./kobart_best_model\"\n",
    "model = BartForConditionalGeneration.from_pretrained(best_checkpoint)\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(best_checkpoint)\n",
    "\n",
    "# ✅ 테스트할 입력 문장\n",
    "test_text = \"이상주의자인 스노우볼과 음흉한 현실주의자인 나폴레온의 싸움은 당연히 나폴레온의 승리로 끝나고 만다.\\n 나폴레온의 독재가 시작되었고, 이제는 철통 같은 규율만을 강조하는 것만으로는 부족했다.\\n 무엇인가 새로운 자극을 주는 대상이 필요한 것이다.\\n 그래서 나폴레온은 추출된 스노우볼을 이용한다.\\n 이제 스노우볼은 모든 동물들로부터 증오의 표적이 된다.\\n 농장에서의 실패는 모두 그의 탓으로 돌려진다.\\n 심지어는 충성도가 떨어지는 동물들은 모두 스노우볼의 앞잡이로 처단된다.\"\n",
    "# ✅ 입력 문장을 토큰화\n",
    "input_ids = tokenizer(test_text, return_tensors=\"pt\", max_length=1024, truncation=True).input_ids\n",
    "\n",
    "# ✅ 모델로 요약 생성\n",
    "summary_ids = model.generate(\n",
    "    input_ids, \n",
    "    max_length=200, \n",
    "    num_beams=5, \n",
    "    early_stopping=True,\n",
    "    temperature=0.5)\n",
    "summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"🔹 원문: \", test_text)\n",
    "print(\"🔹 요약 결과: \", summary_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "## summary \"\" 로 작업용 키값을 설정했더니 줄거리가 없다고 인식하는 경우가 많았음"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
