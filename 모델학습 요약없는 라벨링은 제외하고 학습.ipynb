{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "# âœ… 1ï¸âƒ£ í•™ìŠµ ë°ì´í„° ë¡œë“œ\n",
    "with open(\"labeling_novels_filtered.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    labeled_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… 2ï¸âƒ£ ë°ì´í„°ì…‹ ë³€í™˜\n",
    "train_data = [{\"input\": item[\"description\"], \"target\": item[\"summary\"]} for item in labeled_data]\n",
    "dataset = Dataset.from_dict({\n",
    "    \"input\": [item[\"input\"] for item in train_data],\n",
    "    \"target\": [item[\"target\"] for item in train_data]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "# âœ… 3ï¸âƒ£ KoBART ëª¨ë¸ & í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "model_name = \"gogamza/kobart-summarization\"\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… 4ï¸âƒ£ ë°ì´í„° í† í¬ë‚˜ì´ì§• (Trainerê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜)\n",
    "def preprocess_data(example):\n",
    "    model_inputs = tokenizer(\n",
    "        example[\"input\"], \n",
    "        max_length=1024, \n",
    "        truncation=True, \n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    # ğŸ”¹ \"target\"ì„ \"labels\"ë¡œ ë³€í™˜ í›„ í† í¬ë‚˜ì´ì§•\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            example[\"target\"], \n",
    "            max_length=250, \n",
    "            truncation=True, \n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "282ee381c1ca45ccb7711249e4911506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/217 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/hi/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3970: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# âœ… 5ï¸âƒ£ ë°ì´í„°ì…‹ ë³€í™˜ (batched=Trueë¡œ ì†ë„ ìµœì í™”)\n",
    "tokenized_dataset = dataset.map(preprocess_data, batched=True, remove_columns=[\"input\", \"target\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/hi/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# âœ… 6ï¸âƒ£ í•™ìŠµ ì„¤ì •\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./kobart_summary_finetuned\",\n",
    "    evaluation_strategy=\"epoch\",  # ğŸ”¹ ë§¤ epochë§ˆë‹¤ í‰ê°€\n",
    "    save_strategy=\"epoch\",        # ğŸ”¹ ë§¤ epochë§ˆë‹¤ ì €ì¥\n",
    "    per_device_train_batch_size=2,  # ğŸ”¹ ë°°ì¹˜ í¬ê¸° (GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì • ê°€ëŠ¥)\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=5,  # ğŸ”¹ í•™ìŠµ íšŸìˆ˜ (í•„ìš”ì— ë”°ë¼ ì¡°ì • ê°€ëŠ¥)\n",
    "    weight_decay=0.01,  # ğŸ”¹ ê°€ì¤‘ì¹˜ ê°ì‡  (ê³¼ì í•© ë°©ì§€)\n",
    "    save_total_limit=5,  # ğŸ”¹ ì²´í¬í¬ì¸íŠ¸ ìµœëŒ€ ê°œìˆ˜\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wz/cf10k4857715p70flsxw2g9m0000gn/T/ipykernel_2970/360371609.py:9: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# âœ… 1ï¸âƒ£ ğŸ¤— Datasetsì˜ train_test_split() ì‚¬ìš©\n",
    "dataset_split = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# âœ… 2ï¸âƒ£ train/eval ë°ì´í„°ì…‹ ì„¤ì •\n",
    "train_data = dataset_split[\"train\"]\n",
    "eval_data = dataset_split[\"test\"]\n",
    "\n",
    "# âœ… 3ï¸âƒ£ Trainer ê°ì²´ ìƒì„± (eval_dataset ì¶”ê°€)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,  # ğŸ”¹ í‰ê°€ ë°ì´í„°ì…‹ ì¶”ê°€!\n",
    "    tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wz/cf10k4857715p70flsxw2g9m0000gn/T/ipykernel_2970/2867482875.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# âœ… 7ï¸âƒ£ Trainer ê°ì²´ ìƒì„±\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,   # âœ… í›ˆë ¨ ë°ì´í„°\n",
    "    eval_dataset=eval_data,     # âœ… í‰ê°€ ë°ì´í„° ì¶”ê°€!\n",
    "    tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='435' max='435' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [435/435 03:12, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.817668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.033400</td>\n",
       "      <td>0.783941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.512600</td>\n",
       "      <td>0.816674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.289700</td>\n",
       "      <td>0.840684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.209200</td>\n",
       "      <td>0.862921</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=435, training_loss=0.7102061107240875, metrics={'train_runtime': 192.7244, 'train_samples_per_second': 4.488, 'train_steps_per_second': 2.257, 'total_flos': 527422036377600.0, 'train_loss': 0.7102061107240875, 'epoch': 5.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# âœ… 8ï¸âƒ£ í•™ìŠµ ì‹œì‘ ğŸš€\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… KoBART íŒŒì¸íŠœë‹ ì™„ë£Œ! ëª¨ë¸ì´ './kobart_summary_finetuned'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# âœ… 9ï¸âƒ£ ëª¨ë¸ ì €ì¥\n",
    "model.save_pretrained(\"./kobart_summary_finetuned\")\n",
    "tokenizer.save_pretrained(\"./kobart_summary_finetuned\")\n",
    "\n",
    "print(\"âœ… KoBART íŒŒì¸íŠœë‹ ì™„ë£Œ! ëª¨ë¸ì´ './kobart_summary_finetuned'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ê°€ì¥ ì¢‹ì€ ëª¨ë¸ì´ './kobart_best_model'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast\n",
    "\n",
    "best_checkpoint = \"./kobart_summary_finetuned/checkpoint-174\"  # epoch 2 ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸\n",
    "save_path = \"./kobart_best_model\"\n",
    "\n",
    "# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "model = BartForConditionalGeneration.from_pretrained(best_checkpoint)\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(best_checkpoint)\n",
    "\n",
    "# ì €ì¥\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(f\"âœ… ê°€ì¥ ì¢‹ì€ ëª¨ë¸ì´ '{save_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "/opt/anaconda3/envs/hi/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹ ì›ë¬¸:  ì´ìƒì£¼ì˜ìì¸ ìŠ¤ë…¸ìš°ë³¼ê³¼ ìŒí‰í•œ í˜„ì‹¤ì£¼ì˜ìì¸ ë‚˜í´ë ˆì˜¨ì˜ ì‹¸ì›€ì€ ë‹¹ì—°íˆ ë‚˜í´ë ˆì˜¨ì˜ ìŠ¹ë¦¬ë¡œ ëë‚˜ê³  ë§Œë‹¤.\n",
      " ë‚˜í´ë ˆì˜¨ì˜ ë…ì¬ê°€ ì‹œì‘ë˜ì—ˆê³ , ì´ì œëŠ” ì² í†µ ê°™ì€ ê·œìœ¨ë§Œì„ ê°•ì¡°í•˜ëŠ” ê²ƒë§Œìœ¼ë¡œëŠ” ë¶€ì¡±í–ˆë‹¤.\n",
      " ë¬´ì—‡ì¸ê°€ ìƒˆë¡œìš´ ìê·¹ì„ ì£¼ëŠ” ëŒ€ìƒì´ í•„ìš”í•œ ê²ƒì´ë‹¤.\n",
      " ê·¸ë˜ì„œ ë‚˜í´ë ˆì˜¨ì€ ì¶”ì¶œëœ ìŠ¤ë…¸ìš°ë³¼ì„ ì´ìš©í•œë‹¤.\n",
      " ì´ì œ ìŠ¤ë…¸ìš°ë³¼ì€ ëª¨ë“  ë™ë¬¼ë“¤ë¡œë¶€í„° ì¦ì˜¤ì˜ í‘œì ì´ ëœë‹¤.\n",
      " ë†ì¥ì—ì„œì˜ ì‹¤íŒ¨ëŠ” ëª¨ë‘ ê·¸ì˜ íƒ“ìœ¼ë¡œ ëŒë ¤ì§„ë‹¤.\n",
      " ì‹¬ì§€ì–´ëŠ” ì¶©ì„±ë„ê°€ ë–¨ì–´ì§€ëŠ” ë™ë¬¼ë“¤ì€ ëª¨ë‘ ìŠ¤ë…¸ìš°ë³¼ì˜ ì•ì¡ì´ë¡œ ì²˜ë‹¨ëœë‹¤.\n",
      "ğŸ”¹ ìš”ì•½ ê²°ê³¼:  ì´ìƒì£¼ì˜ìì¸ ìŠ¤ë…¸ìš°ë³¼ê³¼ ìŒí‰í•œ í˜„ì‹¤ì£¼ì˜ìì¸ ë‚˜í´ë ˆì˜¨ì˜ ì‹¸ì›€ì€ ê²°êµ­ ë‚˜í´ë ˆì˜¨ì˜ ìŠ¹ë¦¬ë¡œ ëì´ ë‚˜ì§€ë§Œ, ì² í†µ ê°™ì€ ê·œìœ¨ë§Œì„ ê°•ì¡°í•˜ëŠ” ê²ƒë§Œìœ¼ë¡œëŠ” ë¶€ì¡±í•˜ê³ , ìƒˆë¡œìš´ ìê·¹ì„ ì£¼ëŠ” ëŒ€ìƒì´ í•„ìš”í•œ ìƒí™©ì´ë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast\n",
    "\n",
    "# âœ… ì €ì¥ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "best_checkpoint = \"./kobart_best_model\"\n",
    "model = BartForConditionalGeneration.from_pretrained(best_checkpoint)\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(best_checkpoint)\n",
    "\n",
    "# âœ… í…ŒìŠ¤íŠ¸í•  ì…ë ¥ ë¬¸ì¥\n",
    "test_text = \"ì´ìƒì£¼ì˜ìì¸ ìŠ¤ë…¸ìš°ë³¼ê³¼ ìŒí‰í•œ í˜„ì‹¤ì£¼ì˜ìì¸ ë‚˜í´ë ˆì˜¨ì˜ ì‹¸ì›€ì€ ë‹¹ì—°íˆ ë‚˜í´ë ˆì˜¨ì˜ ìŠ¹ë¦¬ë¡œ ëë‚˜ê³  ë§Œë‹¤.\\n ë‚˜í´ë ˆì˜¨ì˜ ë…ì¬ê°€ ì‹œì‘ë˜ì—ˆê³ , ì´ì œëŠ” ì² í†µ ê°™ì€ ê·œìœ¨ë§Œì„ ê°•ì¡°í•˜ëŠ” ê²ƒë§Œìœ¼ë¡œëŠ” ë¶€ì¡±í–ˆë‹¤.\\n ë¬´ì—‡ì¸ê°€ ìƒˆë¡œìš´ ìê·¹ì„ ì£¼ëŠ” ëŒ€ìƒì´ í•„ìš”í•œ ê²ƒì´ë‹¤.\\n ê·¸ë˜ì„œ ë‚˜í´ë ˆì˜¨ì€ ì¶”ì¶œëœ ìŠ¤ë…¸ìš°ë³¼ì„ ì´ìš©í•œë‹¤.\\n ì´ì œ ìŠ¤ë…¸ìš°ë³¼ì€ ëª¨ë“  ë™ë¬¼ë“¤ë¡œë¶€í„° ì¦ì˜¤ì˜ í‘œì ì´ ëœë‹¤.\\n ë†ì¥ì—ì„œì˜ ì‹¤íŒ¨ëŠ” ëª¨ë‘ ê·¸ì˜ íƒ“ìœ¼ë¡œ ëŒë ¤ì§„ë‹¤.\\n ì‹¬ì§€ì–´ëŠ” ì¶©ì„±ë„ê°€ ë–¨ì–´ì§€ëŠ” ë™ë¬¼ë“¤ì€ ëª¨ë‘ ìŠ¤ë…¸ìš°ë³¼ì˜ ì•ì¡ì´ë¡œ ì²˜ë‹¨ëœë‹¤.\"\n",
    "# âœ… ì…ë ¥ ë¬¸ì¥ì„ í† í°í™”\n",
    "input_ids = tokenizer(test_text, return_tensors=\"pt\", max_length=1024, truncation=True).input_ids\n",
    "\n",
    "# âœ… ëª¨ë¸ë¡œ ìš”ì•½ ìƒì„±\n",
    "summary_ids = model.generate(\n",
    "    input_ids, \n",
    "    max_length=200, \n",
    "    num_beams=5, \n",
    "    early_stopping=True,\n",
    "    temperature=0.5)\n",
    "summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"ğŸ”¹ ì›ë¬¸: \", test_text)\n",
    "print(\"ğŸ”¹ ìš”ì•½ ê²°ê³¼: \", summary_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "## summary \"\" ë¡œ ì‘ì—…ìš© í‚¤ê°’ì„ ì„¤ì •í–ˆë”ë‹ˆ ì¤„ê±°ë¦¬ê°€ ì—†ë‹¤ê³  ì¸ì‹í•˜ëŠ” ê²½ìš°ê°€ ë§ì•˜ìŒ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
