{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "# âœ… 1ï¸âƒ£ í•™ìŠµ ë°ì´í„° ë¡œë“œ\n",
    "with open(\"labeling_novels_filtered.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    labeled_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… 2ï¸âƒ£ ë°ì´í„°ì…‹ ë³€í™˜ (í”„ë¡¬í”„íŠ¸ ì¶”ê°€)\n",
    "train_data = [{\n",
    "    \"input\": f\"ë‹¤ìŒ ë¬¸ì¥ ì¤‘ ì‘ê°€ì˜ ì—°í˜ê³¼ ìˆ˜ìƒì´ë ¥ì€ ì œì™¸í•˜ê³  ìŠ¤í† ë¦¬ë¶€ë¶„ë§Œì„ ìš”ì•½í•´ì£¼ì„¸ìš”. ë“±ì¥ì¸ë¬¼ì˜ ì§ì—…, ìŠ¤í† ë¦¬ ì „ê°œ, ì£¼ìš” ì‚¬ê±´ì„ í¬í•¨í•´ì£¼ì„¸ìš”. :\\n\\n{item['description']}\",\n",
    "    \"target\": item[\"summary\"]\n",
    "} for item in labeled_data]\n",
    "\n",
    "dataset = Dataset.from_dict({\n",
    "    \"input\": [item[\"input\"] for item in train_data],\n",
    "    \"target\": [item[\"target\"] for item in train_data]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "# âœ… 3ï¸âƒ£ KoBART ëª¨ë¸ & í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "model_name = \"gogamza/kobart-summarization\"\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… 4ï¸âƒ£ ë°ì´í„° í† í¬ë‚˜ì´ì§• (Trainerê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜)\n",
    "def preprocess_data(example):\n",
    "    model_inputs = tokenizer(\n",
    "        example[\"input\"], \n",
    "        max_length=1024, \n",
    "        truncation=True, \n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    # ğŸ”¹ \"target\"ì„ \"labels\"ë¡œ ë³€í™˜ í›„ í† í¬ë‚˜ì´ì§•\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            example[\"target\"], \n",
    "            max_length=250, \n",
    "            truncation=True, \n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d9ed869590847d1bc7fa4bdb5248c73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/217 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/hi/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3970: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# âœ… 5ï¸âƒ£ ë°ì´í„°ì…‹ ë³€í™˜ (batched=Trueë¡œ ì†ë„ ìµœì í™”)\n",
    "tokenized_dataset = dataset.map(preprocess_data, batched=True, remove_columns=[\"input\", \"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/hi/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# âœ… 6ï¸âƒ£ í•™ìŠµ ì„¤ì •\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./kobart_summary_finetuned\",\n",
    "    evaluation_strategy=\"epoch\",  # ğŸ”¹ ë§¤ epochë§ˆë‹¤ í‰ê°€\n",
    "    save_strategy=\"epoch\",        # ğŸ”¹ ë§¤ epochë§ˆë‹¤ ì €ì¥\n",
    "    per_device_train_batch_size=2,  # ğŸ”¹ ë°°ì¹˜ í¬ê¸° (GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì • ê°€ëŠ¥)\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=5,  # ğŸ”¹ í•™ìŠµ íšŸìˆ˜ (í•„ìš”ì— ë”°ë¼ ì¡°ì • ê°€ëŠ¥)\n",
    "    weight_decay=0.01,  # ğŸ”¹ ê°€ì¤‘ì¹˜ ê°ì‡  (ê³¼ì í•© ë°©ì§€)\n",
    "    save_total_limit=5,  # ğŸ”¹ ì²´í¬í¬ì¸íŠ¸ ìµœëŒ€ ê°œìˆ˜\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wz/cf10k4857715p70flsxw2g9m0000gn/T/ipykernel_2894/3899536195.py:9: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# âœ… 7ï¸âƒ£ ğŸ¤— Datasetsì˜ train_test_split() ì‚¬ìš©\n",
    "dataset_split = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# âœ… 8ï¸âƒ£ train/eval ë°ì´í„°ì…‹ ì„¤ì •\n",
    "train_data = dataset_split[\"train\"]\n",
    "eval_data = dataset_split[\"test\"]\n",
    "\n",
    "# âœ… 9ï¸âƒ£ Trainer ê°ì²´ ìƒì„± (eval_dataset ì¶”ê°€)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,  # ğŸ”¹ í‰ê°€ ë°ì´í„°ì…‹ ì¶”ê°€!\n",
    "    tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='435' max='435' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [435/435 03:11, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.811702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.022500</td>\n",
       "      <td>0.783771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.513200</td>\n",
       "      <td>0.811638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.297200</td>\n",
       "      <td>0.836755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.213300</td>\n",
       "      <td>0.850627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=435, training_loss=0.7107568982003749, metrics={'train_runtime': 191.9181, 'train_samples_per_second': 4.507, 'train_steps_per_second': 2.267, 'total_flos': 527422036377600.0, 'train_loss': 0.7107568982003749, 'epoch': 5.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# âœ… 10 í•™ìŠµ ì‹œì‘ ğŸš€\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… KoBART íŒŒì¸íŠœë‹ ì™„ë£Œ! ëª¨ë¸ì´ './kobart_summary_finetuned'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# âœ… 9ï¸âƒ£ ëª¨ë¸ ì €ì¥\n",
    "model.save_pretrained(\"./kobart_summary_finetuned\")\n",
    "tokenizer.save_pretrained(\"./kobart_summary_finetuned\")\n",
    "\n",
    "print(\"âœ… KoBART íŒŒì¸íŠœë‹ ì™„ë£Œ! ëª¨ë¸ì´ './kobart_summary_finetuned'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ê°€ì¥ ì¢‹ì€ ëª¨ë¸ì´ './kobart_best_model'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast\n",
    "\n",
    "best_checkpoint = \"./kobart_summary_finetuned/checkpoint-174\"  # epoch 2 ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸\n",
    "save_path = \"./kobart_best_model\"\n",
    "\n",
    "# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "model = BartForConditionalGeneration.from_pretrained(best_checkpoint)\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(best_checkpoint)\n",
    "\n",
    "# ì €ì¥\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(f\"âœ… ê°€ì¥ ì¢‹ì€ ëª¨ë¸ì´ '{save_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹ ì›ë¬¸:  ë…ìë¥¼ ê³µëª¨ìë¡œ ëŒì–´ë“¤ì¸ ì±„ ê´€ì°°í•˜ëŠ” ìë§¤ì˜ í•˜ë£»ë°¤!\n",
      "\n",
      "ë¬´ë¼ì¹´ë¯¸ í•˜ë£¨í‚¤ ë°ë·” 25ì£¼ë…„ ê¸°ë… ì¥í¸ì†Œì„¤ ã€ì• í”„í„° ë‹¤í¬ã€.\n",
      " 1979ë…„ ã€Šë°”ëŒì˜ ë…¸ë˜ë¥¼ ë“¤ì–´ë¼ã€‹ë¡œ êµ°ì¡°ì‹ ì¸ë¬¸í•™ìƒì„ ìˆ˜ìƒí•˜ë©° ë°ë·”í•œ ì´í›„ ë…ìë“¤ì˜ ì‚¬ë‘ì„ ë°›ì•„ì˜¨ ë¬´ë¼ì¹´ë¯¸ í•˜ë£¨í‚¤.\n",
      " ê·¸ê°€ ë“±ë‹¨ 25ì£¼ë…„ì„ ë§ëŠ” í•´ì— ë°œí‘œí•œ 11ë²ˆì§¸ ì¥í¸ì†Œì„¤ë¡œ, ë°œí‘œ ì‹œê¸°ì ìœ¼ë¡œëŠ” ã€Ší•´ë³€ì˜ ì¹´í”„ì¹´ã€‹ì™€ ã€Š1Q84ã€‹ ì‚¬ì´ì—, ë³¼ë¥¨ìœ¼ë¡œëŠ” ã€Šêµ­ê²½ì˜ ë‚¨ìª½, íƒœì–‘ì˜ ì„œìª½ã€‹, ã€ŠìŠ¤í‘¸íŠ¸ë‹ˆí¬ì˜ ì—°ì¸ã€‹ê³¼ ê°™ì€ ì¥í¸ì†Œì„¤ ì˜†ì— ë‚˜ë€íˆ ìœ„ì¹˜í•œë‹¤.\n",
      "\n",
      "\n",
      "ì‘í’ˆì€ ìì •ì´ ê°€ê¹Œìš´ í•œë°¤ì—ì„œë¶€í„° ìƒˆë‚ ì´ ë°ì•„ì˜¤ëŠ” ì•„ì¹¨ê¹Œì§€ ì¼ê³± ì‹œê°„ ë™ì•ˆ ë²Œì–´ì§€ëŠ” ë°±ì„¤ ê³µì£¼ì²˜ëŸ¼ ì˜ˆìœ ì–¸ë‹ˆ â€˜ì—ë¦¬â€™ì™€ ì”©ì”©í•œ ì–‘ì¹˜ê¸° ëª©ë™ ê°™ì€ ë™ìƒ â€˜ë§ˆë¦¬â€™, ë‘ ìë§¤ì˜ ì´ì•¼ê¸°ë¥¼ ë‹´ê³  ìˆë‹¤.\n",
      " â€˜ìš°ë¦¬â€™ë¼ê³  ëª…ëª…ëœ ì¹´ë©”ë¼ì˜ ì‹œì„ ì´ ì´ì•¼ê¸°ë¥¼ ì£¼ë„í•œë‹¤.\n",
      " ë†’ì€ ê³³ì—ì„œ ì¡°ê°í•˜ëŠ”ê°€ í•˜ë©´, ë•Œë¡œëŠ” ê·¼ì ‘í•˜ì—¬ í´ë¡œì¦ˆì—…ì„ ì‹œë„í•˜ë©° ì˜í™”ì˜ ì¥ë©´ë“¤ì²˜ëŸ¼ ì—ë¦¬ì˜ ë°¤ê³¼ ë§ˆë¦¬ì˜ ë°¤ì„ êµëŒ€ë¡œ ë¹„ì¶”ëŠ” ë™ì•ˆ ì‘ê°€ëŠ” ì–´ë– í•œ ì‹ìœ¼ë¡œë“  ì„¤ëª…ì„ ë”í•˜ê±°ë‚˜ ê°œì…í•˜ì§€ ì•ŠëŠ”ë‹¤.\n",
      " ê·¸ì € ë…ìë“¤ì„ ë°¤ê³¼ ì–´ë‘ ì˜ ì´ë¯¸ì§€ë¡œ ì•ˆë‚´í•  ë¿ì´ë‹¤.\n",
      " \n",
      "\n",
      "íŒ¨ë°€ë¦¬ë ˆìŠ¤í† ë‘ì—ì„œ í˜¼ì ì±…ì„ ì½ê³  ìˆëŠ” ë§ˆë¦¬ì—ê²Œ ì Šì€ ë‚¨ìê°€ ë‹¤ê°€ì™€ ë§ì„ ê±´ë‹¤.\n",
      " â€œí˜¹ì‹œ ì•„ì‚¬ì´ ì—ë¦¬ ë™ìƒ ì•„ëƒ? ì „ì— ìš°ë¦¬ í•œ ë²ˆ ë§Œë‚¬ì§€?â€ í•˜ë£»ë°¤ ë™ì•ˆ ë§ˆë¦¬ëŠ” ë‹¤ì–‘í•œ ì‚¬ëŒë“¤ê³¼ ë§Œë‚˜ ëŒ€í™”ë¥¼ ì£¼ê³ ë°›ëŠ”ë‹¤.\n",
      " ì£¼ë¡œ ì ì„ ë¹¼ì•—ê¸´ ì±„ ë°¤ì„ ì§€ìƒˆìš°ê³  ìˆëŠ” ì‚¬ëŒë“¤ì´ë‹¤.\n",
      " ë°´ë“œ ì£¼ì, ì¤‘êµ­ì¸ ì°½ë¶€, ëŸ¬ë¸Œí˜¸í…” ìŠ¤íƒœí”„â€¦â€¦ ë§ˆë¦¬ëŠ” ì™œ ë°¤ì˜ ê±°ë¦¬ë¥¼ ë°©í™©í•˜ëŠ” ê±¸ê¹Œ? ë°˜ëŒ€ë¡œ ì–¸ë‹ˆ ì—ë¦¬ëŠ” ì™œ ë‘ ë‹¬ì§¸ ê¹Šì€ ì ì— ë¹ ì ¸ ìˆëŠ” ê±¸ê¹Œ? ë°¤ì„ ê±·ëŠ” ì‚¬ëŒë“¤ì€ ë‹¤ë“¤ ì–´ë””ì—ì„œ ë„ë§ì¹˜ê³  ì‹¶ì€ ê±¸ê¹Œ? ë‹¤ì–‘í•œ ìˆ˜ìˆ˜ê»˜ë¼ë¥¼ ë¨¸ê¸ˆì€ ì°°ë‚˜ë“¤ì´ ìŠ¤ë¦´ ìˆê²Œ íë¥´ê³ , ë°¤ 11ì‹œ 52ë¶„ì— ì‹œì‘í•œ ì´ì•¼ê¸°ëŠ” ìµì¼ 6ì‹œ 52ë¶„ì„ ê¸°ì ìœ¼ë¡œ ë§‰ì„ ë‚´ë¦°ë‹¤.\n",
      "\n",
      "ğŸ”¹ ìš”ì•½ ê²°ê³¼:  ìì •ì´ ê°€ê¹Œìš´ í•œë°¤ì—ì„œë¶€í„° ìƒˆë‚ ì´ ë°ì•„ì˜¤ëŠ” ì•„ì¹¨ê¹Œì§€ ì¼ê³± ì‹œê°„ ë™ì•ˆ ë²Œì–´ì§€ëŠ” ë°±ì„¤ ê³µì£¼ì²˜ëŸ¼ ì˜ˆìœ ì–¸ë‹ˆ â€˜ì—ë¦¬â€™ì™€ ì”©ì”©í•œ ì–‘ì¹˜ê¸° ëª©ë™ ê°™ì€ ë™ìƒ â€˜ë§ˆë¦¬â€™ì˜ ì´ì•¼ê¸°. ë‘ ìë§¤ì˜ ì´ì•¼ê¸°ë¥¼ ê·¸ë¦° ì¥í¸ì†Œì„¤ì´ë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast\n",
    "\n",
    "# âœ… ì €ì¥ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "best_checkpoint = \"./kobart_best_model\"\n",
    "model = BartForConditionalGeneration.from_pretrained(best_checkpoint)\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(best_checkpoint)\n",
    "\n",
    "# âœ… í…ŒìŠ¤íŠ¸í•  ì…ë ¥ ë¬¸ì¥\n",
    "test_text = \"ë…ìë¥¼ ê³µëª¨ìë¡œ ëŒì–´ë“¤ì¸ ì±„ ê´€ì°°í•˜ëŠ” ìë§¤ì˜ í•˜ë£»ë°¤!\\n\\në¬´ë¼ì¹´ë¯¸ í•˜ë£¨í‚¤ ë°ë·” 25ì£¼ë…„ ê¸°ë… ì¥í¸ì†Œì„¤ ã€ì• í”„í„° ë‹¤í¬ã€.\\n 1979ë…„ ã€Šë°”ëŒì˜ ë…¸ë˜ë¥¼ ë“¤ì–´ë¼ã€‹ë¡œ êµ°ì¡°ì‹ ì¸ë¬¸í•™ìƒì„ ìˆ˜ìƒí•˜ë©° ë°ë·”í•œ ì´í›„ ë…ìë“¤ì˜ ì‚¬ë‘ì„ ë°›ì•„ì˜¨ ë¬´ë¼ì¹´ë¯¸ í•˜ë£¨í‚¤.\\n ê·¸ê°€ ë“±ë‹¨ 25ì£¼ë…„ì„ ë§ëŠ” í•´ì— ë°œí‘œí•œ 11ë²ˆì§¸ ì¥í¸ì†Œì„¤ë¡œ, ë°œí‘œ ì‹œê¸°ì ìœ¼ë¡œëŠ” ã€Ší•´ë³€ì˜ ì¹´í”„ì¹´ã€‹ì™€ ã€Š1Q84ã€‹ ì‚¬ì´ì—, ë³¼ë¥¨ìœ¼ë¡œëŠ” ã€Šêµ­ê²½ì˜ ë‚¨ìª½, íƒœì–‘ì˜ ì„œìª½ã€‹, ã€ŠìŠ¤í‘¸íŠ¸ë‹ˆí¬ì˜ ì—°ì¸ã€‹ê³¼ ê°™ì€ ì¥í¸ì†Œì„¤ ì˜†ì— ë‚˜ë€íˆ ìœ„ì¹˜í•œë‹¤.\\n\\n\\nì‘í’ˆì€ ìì •ì´ ê°€ê¹Œìš´ í•œë°¤ì—ì„œë¶€í„° ìƒˆë‚ ì´ ë°ì•„ì˜¤ëŠ” ì•„ì¹¨ê¹Œì§€ ì¼ê³± ì‹œê°„ ë™ì•ˆ ë²Œì–´ì§€ëŠ” ë°±ì„¤ ê³µì£¼ì²˜ëŸ¼ ì˜ˆìœ ì–¸ë‹ˆ â€˜ì—ë¦¬â€™ì™€ ì”©ì”©í•œ ì–‘ì¹˜ê¸° ëª©ë™ ê°™ì€ ë™ìƒ â€˜ë§ˆë¦¬â€™, ë‘ ìë§¤ì˜ ì´ì•¼ê¸°ë¥¼ ë‹´ê³  ìˆë‹¤.\\n â€˜ìš°ë¦¬â€™ë¼ê³  ëª…ëª…ëœ ì¹´ë©”ë¼ì˜ ì‹œì„ ì´ ì´ì•¼ê¸°ë¥¼ ì£¼ë„í•œë‹¤.\\n ë†’ì€ ê³³ì—ì„œ ì¡°ê°í•˜ëŠ”ê°€ í•˜ë©´, ë•Œë¡œëŠ” ê·¼ì ‘í•˜ì—¬ í´ë¡œì¦ˆì—…ì„ ì‹œë„í•˜ë©° ì˜í™”ì˜ ì¥ë©´ë“¤ì²˜ëŸ¼ ì—ë¦¬ì˜ ë°¤ê³¼ ë§ˆë¦¬ì˜ ë°¤ì„ êµëŒ€ë¡œ ë¹„ì¶”ëŠ” ë™ì•ˆ ì‘ê°€ëŠ” ì–´ë– í•œ ì‹ìœ¼ë¡œë“  ì„¤ëª…ì„ ë”í•˜ê±°ë‚˜ ê°œì…í•˜ì§€ ì•ŠëŠ”ë‹¤.\\n ê·¸ì € ë…ìë“¤ì„ ë°¤ê³¼ ì–´ë‘ ì˜ ì´ë¯¸ì§€ë¡œ ì•ˆë‚´í•  ë¿ì´ë‹¤.\\n \\n\\níŒ¨ë°€ë¦¬ë ˆìŠ¤í† ë‘ì—ì„œ í˜¼ì ì±…ì„ ì½ê³  ìˆëŠ” ë§ˆë¦¬ì—ê²Œ ì Šì€ ë‚¨ìê°€ ë‹¤ê°€ì™€ ë§ì„ ê±´ë‹¤.\\n â€œí˜¹ì‹œ ì•„ì‚¬ì´ ì—ë¦¬ ë™ìƒ ì•„ëƒ? ì „ì— ìš°ë¦¬ í•œ ë²ˆ ë§Œë‚¬ì§€?â€ í•˜ë£»ë°¤ ë™ì•ˆ ë§ˆë¦¬ëŠ” ë‹¤ì–‘í•œ ì‚¬ëŒë“¤ê³¼ ë§Œë‚˜ ëŒ€í™”ë¥¼ ì£¼ê³ ë°›ëŠ”ë‹¤.\\n ì£¼ë¡œ ì ì„ ë¹¼ì•—ê¸´ ì±„ ë°¤ì„ ì§€ìƒˆìš°ê³  ìˆëŠ” ì‚¬ëŒë“¤ì´ë‹¤.\\n ë°´ë“œ ì£¼ì, ì¤‘êµ­ì¸ ì°½ë¶€, ëŸ¬ë¸Œí˜¸í…” ìŠ¤íƒœí”„â€¦â€¦ ë§ˆë¦¬ëŠ” ì™œ ë°¤ì˜ ê±°ë¦¬ë¥¼ ë°©í™©í•˜ëŠ” ê±¸ê¹Œ? ë°˜ëŒ€ë¡œ ì–¸ë‹ˆ ì—ë¦¬ëŠ” ì™œ ë‘ ë‹¬ì§¸ ê¹Šì€ ì ì— ë¹ ì ¸ ìˆëŠ” ê±¸ê¹Œ? ë°¤ì„ ê±·ëŠ” ì‚¬ëŒë“¤ì€ ë‹¤ë“¤ ì–´ë””ì—ì„œ ë„ë§ì¹˜ê³  ì‹¶ì€ ê±¸ê¹Œ? ë‹¤ì–‘í•œ ìˆ˜ìˆ˜ê»˜ë¼ë¥¼ ë¨¸ê¸ˆì€ ì°°ë‚˜ë“¤ì´ ìŠ¤ë¦´ ìˆê²Œ íë¥´ê³ , ë°¤ 11ì‹œ 52ë¶„ì— ì‹œì‘í•œ ì´ì•¼ê¸°ëŠ” ìµì¼ 6ì‹œ 52ë¶„ì„ ê¸°ì ìœ¼ë¡œ ë§‰ì„ ë‚´ë¦°ë‹¤.\\n\"\n",
    "# âœ… ì…ë ¥ ë¬¸ì¥ì„ í† í°í™”\n",
    "input_ids = tokenizer(\n",
    "    test_text, \n",
    "    return_tensors=\"pt\", \n",
    "    max_length=1024, \n",
    "    truncation=True\n",
    "    \n",
    "    ).input_ids\n",
    "\n",
    "# âœ… ëª¨ë¸ë¡œ ìš”ì•½ ìƒì„±\n",
    "summary_ids = model.generate(\n",
    "    input_ids, \n",
    "    max_length=200, \n",
    "    num_beams=5, \n",
    "    early_stopping=True,\n",
    "    repetition_penalty=1.5,\n",
    "    length_penalty=1.5,\n",
    "    temperature=0.8)\n",
    "summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"ğŸ”¹ ì›ë¬¸: \", test_text)\n",
    "print(\"ğŸ”¹ ìš”ì•½ ê²°ê³¼: \", summary_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "## summary \"\" ë¡œ ì‘ì—…ìš© í‚¤ê°’ì„ ì„¤ì •í–ˆë”ë‹ˆ ì¤„ê±°ë¦¬ê°€ ì—†ë‹¤ê³  ì¸ì‹í•˜ëŠ” ê²½ìš°ê°€ ë§ì•˜ìŒ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
